{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c618949",
   "metadata": {},
   "source": [
    "In the following we will go through some essential topics that forms the basis of performing data analysis on textual data. In order to perform data analysis, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca0722b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# The basic library we will start using is nltk that offers numerous tools for natural language processing\n",
    "# In order to import it, first you have to install it as with other packages before\n",
    "# conda install -c anaconda nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# A very extensive treatment on using nltk is available in an free online book\n",
    "# If you plan to work with textual data extensively, I recommend checking that\n",
    "# There are numerous other libraries that you can use, such as spaCy\n",
    "# You can get introduced to that in a DataCamp course Feature Engineering for NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef2c00",
   "metadata": {},
   "source": [
    "### Basic text processing\n",
    "We start with somebasic techniques that you want to perform on any piece of textual information: text preprocessing.\n",
    "You are already familiar with strings in python, and can perform some basic operations, but here we focus on the tasks that will help us to extract information in a meaningful way. I will only focus on the most important tasks, for a more extensive discussion you may consult the above mentioned resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe3a09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text mining usually involves the process of structuring the input text. the overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (nlp) and analytical methods.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some simple basic things we already know about preprocessing text data (strings)\n",
    "# include for example changing text to lowercase latters\n",
    "\n",
    "# Take an example string\n",
    "text_ex = \"Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\"\n",
    "\n",
    "# We can change it to lowercase \n",
    "# (this can be important, as from the perspective of meaning, Text and text should count as the same)\n",
    "\n",
    "text_ex.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f8a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: an important task in working with text is matching patterns\n",
    "# So for example if you want to extract email addresses from a large text, you need to find\n",
    "# things of the form (something)@(something).(something), where (something can be anything)\n",
    "# To do these types of tasks, you want to utilize so-called regular epxressions\n",
    "# We do not have the time to cover those here, but I would definitely recommend checking out\n",
    "# the course 'Regular Expressions in Python' in Datacamp, this can be very useful in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949ee6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jmezei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first sting what sometimes we want to do is to take a text and cut it into meaningful units.\n",
    "# The unit of analysis can be sentence or word typically\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# When we use nltk, in most cases we need to download extra rescources to perform text processing\n",
    "# You only have to run this once, it will be downloaded and be loaded together with nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c915f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '.', 'The', 'overarching', 'goal', 'is', ',', 'essentially', ',', 'to', 'turn', 'text', 'into', 'data', 'for', 'analysis', ',', 'via', 'application', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'analytical', 'methods', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can create a list of words\n",
    "\n",
    "word_text = word_tokenize(text_ex)\n",
    "print(word_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a64a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text           1\n",
      "mining         1\n",
      "usually        1\n",
      "involves       1\n",
      "the            2\n",
      "process        1\n",
      "of             2\n",
      "structuring    1\n",
      "input          1\n",
      "text           2\n",
      ".              2\n",
      "The            1\n",
      "overarching    1\n",
      "goal           1\n",
      "is             1\n",
      ",              3\n",
      "essentially    1\n",
      "to             1\n",
      "turn           1\n",
      "into           1\n",
      "data           1\n",
      "for            1\n",
      "analysis       1\n",
      "via            1\n",
      "application    1\n",
      "natural        1\n",
      "language       1\n",
      "processing     1\n",
      "(              1\n",
      "NLP            1\n",
      ")              1\n",
      "and            1\n",
      "analytical     1\n",
      "methods        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We can also creat a Series of the count of occurences of each word\n",
    "\n",
    "print(pd.Series(dict(nltk.FreqDist(word_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96311140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text mining usually involves the process of structuring the input text.', 'The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.']\n"
     ]
    }
   ],
   "source": [
    "# Or we can create a list of sentences\n",
    "# The algorithm correctlyfinds the two sentences\n",
    "\n",
    "sent_text = sent_tokenize(text_ex)\n",
    "print(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f3d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jmezei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another important thing is to filter out stopwords\n",
    "# stopwords of a language is the list of most commonly used words, that you typically want to exclude\n",
    "# from any further analysis, as they do not contain any important information, as any random text should have these words\n",
    "\n",
    "# We need to download also this first to use it\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# We can check what are the stopwords in the English language\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2812a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olla',\n",
       " 'olen',\n",
       " 'olet',\n",
       " 'on',\n",
       " 'olemme',\n",
       " 'olette',\n",
       " 'ovat',\n",
       " 'ole',\n",
       " 'oli',\n",
       " 'olisi',\n",
       " 'olisit',\n",
       " 'olisin',\n",
       " 'olisimme',\n",
       " 'olisitte',\n",
       " 'olisivat',\n",
       " 'olit',\n",
       " 'olin',\n",
       " 'olimme',\n",
       " 'olitte',\n",
       " 'olivat',\n",
       " 'ollut',\n",
       " 'olleet',\n",
       " 'en',\n",
       " 'et',\n",
       " 'ei',\n",
       " 'emme',\n",
       " 'ette',\n",
       " 'eivät',\n",
       " 'minä',\n",
       " 'minun',\n",
       " 'minut',\n",
       " 'minua',\n",
       " 'minussa',\n",
       " 'minusta',\n",
       " 'minuun',\n",
       " 'minulla',\n",
       " 'minulta',\n",
       " 'minulle',\n",
       " 'sinä',\n",
       " 'sinun',\n",
       " 'sinut',\n",
       " 'sinua',\n",
       " 'sinussa',\n",
       " 'sinusta',\n",
       " 'sinuun',\n",
       " 'sinulla',\n",
       " 'sinulta',\n",
       " 'sinulle',\n",
       " 'hän',\n",
       " 'hänen',\n",
       " 'hänet',\n",
       " 'häntä',\n",
       " 'hänessä',\n",
       " 'hänestä',\n",
       " 'häneen',\n",
       " 'hänellä',\n",
       " 'häneltä',\n",
       " 'hänelle',\n",
       " 'me',\n",
       " 'meidän',\n",
       " 'meidät',\n",
       " 'meitä',\n",
       " 'meissä',\n",
       " 'meistä',\n",
       " 'meihin',\n",
       " 'meillä',\n",
       " 'meiltä',\n",
       " 'meille',\n",
       " 'te',\n",
       " 'teidän',\n",
       " 'teidät',\n",
       " 'teitä',\n",
       " 'teissä',\n",
       " 'teistä',\n",
       " 'teihin',\n",
       " 'teillä',\n",
       " 'teiltä',\n",
       " 'teille',\n",
       " 'he',\n",
       " 'heidän',\n",
       " 'heidät',\n",
       " 'heitä',\n",
       " 'heissä',\n",
       " 'heistä',\n",
       " 'heihin',\n",
       " 'heillä',\n",
       " 'heiltä',\n",
       " 'heille',\n",
       " 'tämä',\n",
       " 'tämän',\n",
       " 'tätä',\n",
       " 'tässä',\n",
       " 'tästä',\n",
       " 'tähän',\n",
       " 'tallä',\n",
       " 'tältä',\n",
       " 'tälle',\n",
       " 'tänä',\n",
       " 'täksi',\n",
       " 'tuo',\n",
       " 'tuon',\n",
       " 'tuotä',\n",
       " 'tuossa',\n",
       " 'tuosta',\n",
       " 'tuohon',\n",
       " 'tuolla',\n",
       " 'tuolta',\n",
       " 'tuolle',\n",
       " 'tuona',\n",
       " 'tuoksi',\n",
       " 'se',\n",
       " 'sen',\n",
       " 'sitä',\n",
       " 'siinä',\n",
       " 'siitä',\n",
       " 'siihen',\n",
       " 'sillä',\n",
       " 'siltä',\n",
       " 'sille',\n",
       " 'sinä',\n",
       " 'siksi',\n",
       " 'nämä',\n",
       " 'näiden',\n",
       " 'näitä',\n",
       " 'näissä',\n",
       " 'näistä',\n",
       " 'näihin',\n",
       " 'näillä',\n",
       " 'näiltä',\n",
       " 'näille',\n",
       " 'näinä',\n",
       " 'näiksi',\n",
       " 'nuo',\n",
       " 'noiden',\n",
       " 'noita',\n",
       " 'noissa',\n",
       " 'noista',\n",
       " 'noihin',\n",
       " 'noilla',\n",
       " 'noilta',\n",
       " 'noille',\n",
       " 'noina',\n",
       " 'noiksi',\n",
       " 'ne',\n",
       " 'niiden',\n",
       " 'niitä',\n",
       " 'niissä',\n",
       " 'niistä',\n",
       " 'niihin',\n",
       " 'niillä',\n",
       " 'niiltä',\n",
       " 'niille',\n",
       " 'niinä',\n",
       " 'niiksi',\n",
       " 'kuka',\n",
       " 'kenen',\n",
       " 'kenet',\n",
       " 'ketä',\n",
       " 'kenessä',\n",
       " 'kenestä',\n",
       " 'keneen',\n",
       " 'kenellä',\n",
       " 'keneltä',\n",
       " 'kenelle',\n",
       " 'kenenä',\n",
       " 'keneksi',\n",
       " 'ketkä',\n",
       " 'keiden',\n",
       " 'ketkä',\n",
       " 'keitä',\n",
       " 'keissä',\n",
       " 'keistä',\n",
       " 'keihin',\n",
       " 'keillä',\n",
       " 'keiltä',\n",
       " 'keille',\n",
       " 'keinä',\n",
       " 'keiksi',\n",
       " 'mikä',\n",
       " 'minkä',\n",
       " 'minkä',\n",
       " 'mitä',\n",
       " 'missä',\n",
       " 'mistä',\n",
       " 'mihin',\n",
       " 'millä',\n",
       " 'miltä',\n",
       " 'mille',\n",
       " 'minä',\n",
       " 'miksi',\n",
       " 'mitkä',\n",
       " 'joka',\n",
       " 'jonka',\n",
       " 'jota',\n",
       " 'jossa',\n",
       " 'josta',\n",
       " 'johon',\n",
       " 'jolla',\n",
       " 'jolta',\n",
       " 'jolle',\n",
       " 'jona',\n",
       " 'joksi',\n",
       " 'jotka',\n",
       " 'joiden',\n",
       " 'joita',\n",
       " 'joissa',\n",
       " 'joista',\n",
       " 'joihin',\n",
       " 'joilla',\n",
       " 'joilta',\n",
       " 'joille',\n",
       " 'joina',\n",
       " 'joiksi',\n",
       " 'että',\n",
       " 'ja',\n",
       " 'jos',\n",
       " 'koska',\n",
       " 'kuin',\n",
       " 'mutta',\n",
       " 'niin',\n",
       " 'sekä',\n",
       " 'sillä',\n",
       " 'tai',\n",
       " 'vaan',\n",
       " 'vai',\n",
       " 'vaikka',\n",
       " 'kanssa',\n",
       " 'mukaan',\n",
       " 'noin',\n",
       " 'poikki',\n",
       " 'yli',\n",
       " 'kun',\n",
       " 'niin',\n",
       " 'nyt',\n",
       " 'itse']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other languages are also available\n",
    "stopwords.words(\"finnish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4c21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '.', 'the', 'overarching', 'goal', 'is', ',', 'essentially', ',', 'to', 'turn', 'text', 'into', 'data', 'for', 'analysis', ',', 'via', 'application', 'of', 'natural', 'language', 'processing', '(', 'nlp', ')', 'and', 'analytical', 'methods', '.']\n",
      "['text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', '.', 'overarching', 'goal', ',', 'essentially', ',', 'turn', 'text', 'data', 'analysis', ',', 'via', 'application', 'natural', 'language', 'processing', '(', 'nlp', ')', 'analytical', 'methods', '.']\n"
     ]
    }
   ],
   "source": [
    "# For our example we can set the stopwords\n",
    "\n",
    "stop_words = list(stopwords.words(\"english\"))\n",
    "\n",
    "# We can change our string to lowercase and create list of words\n",
    "\n",
    "word_list = word_tokenize(text_ex.lower())\n",
    "\n",
    "word_list_filtered = []\n",
    "\n",
    "# We collect every word which is not a stopword in a new list\n",
    "\n",
    "for word in word_list:\n",
    "    if word not in stop_words:\n",
    "        word_list_filtered.append(word)\n",
    "        \n",
    "print(word_list)\n",
    "        \n",
    "print(word_list_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2f3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', '.', 'overarching', 'goal', ',', 'essentially', ',', 'turn', 'text', 'data', 'analysis', ',', 'via', 'application', 'natural', 'language', 'processing', '(', 'nlp', ')', 'analytical', 'methods', '.']\n",
      "['mining', 'usually', 'involves', 'process', 'structuring', 'input', '.', 'overarching', 'goal', ',', 'essentially', ',', 'turn', 'data', 'analysis', ',', 'via', 'application', 'natural', 'language', 'processing', '(', 'nlp', ')', 'analytical', 'methods', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can also extend the list of stopwords depending on the text sources we analysing. \n",
    "# For example, if we have a lot of movie reviews, we probably want to remove the word 'movie' from all the reviews\n",
    "# as it does not contain any new information, we know that we analyse movie reviews\n",
    "# In our example we can decide that text is also a stopword\n",
    "\n",
    "# We append 'text' to the list of stopwords\n",
    "stop_words += ['text']\n",
    "\n",
    "word_list_filtered_new = []\n",
    "\n",
    "# We can collect the list again \n",
    "\n",
    "for word in word_list:\n",
    "    if word not in stop_words:\n",
    "        word_list_filtered_new.append(word)\n",
    "        \n",
    "print(word_list_filtered)\n",
    "        \n",
    "print(word_list_filtered_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338976d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next important thing is to take care problems when different forms of the same word are not combined in further analysis\n",
    "# Typically, if we have the words 'car' and 'cars', we would like them to be combined, as they refer to the same concept\n",
    "# just one is in a plural form\n",
    "# There are different ways to solve this problem, the two most widely used onesare stemming and lemmatization\n",
    "\n",
    "# The ide of stemming is very simple: just cut of the end of the word\n",
    "# This is appropriate in many situations, but this will not give you the rootof the word in many cases\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# we always need to initialize a stemmer object first\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# For example let's consider the following list of words\n",
    "\n",
    "word_list = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "# Stemming wourld result in the following\n",
    "# As you can see, it at least manages to differentiate between a verb (run) \n",
    "# and noun (runner) and does not result in the same word\n",
    "\n",
    "[stemmer.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our original eample sentence:\n",
    "print(word_text)\n",
    "print([stemmer.stem(word) for word in word_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization is a much more complex process\n",
    "# It does not simply reduce the word word reduction, but considers properties of the language and grammar \n",
    "# and also how the word is used. For example, the lemma of 'meeting' \n",
    "# might be 'meet' or 'meeting' depending on its use in a sentence.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# We initialize the object\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# As an example, we can see how it works on the word 'mice' compared to stemming\n",
    "# As you can see, it understands that this is the plural of mouse, and returns that as the base form of the word, \n",
    "# which would be used in the rest of the analysis, while stemming has no effect on the word, as it does not find anything to cut\n",
    "\n",
    "print(stemmer.stem('mice'))\n",
    "print(lemmatizer.lemmatize('mice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the above list of words\n",
    "\n",
    "[lemmatizer.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And our original sentences\n",
    "\n",
    "lemmatizer_1 = WordNetLemmatizer()\n",
    "\n",
    "print(word_text)\n",
    "print([lemmatizer_1.lemmatize(word) for word in word_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58218d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another important example that can be important in many applications is to tag parts of speech\n",
    "# i.e. an algorithm that can identify whether a word is a noun, verb etc.\n",
    "\n",
    "# We need to download another component first\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.pos_tag(word_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ef6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to understand the meaning of the abbreviations\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f907ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this we can now perform correct lemmatization\n",
    "# We still need to create an extra fucntion as the POS tag above is not usable in the lemmatizer\n",
    "# We extract the first character \n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    if pos_tag == 'J':\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif pos_tag == 'N':\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif pos_tag == 'V':\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif pos_tag == 'R':\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "print(word_text)\n",
    "print([lemmatizer_1.lemmatize(word, get_wordnet_pos(word)) for word in word_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We can check what components of nltk we have already downloaded\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f74d74",
   "metadata": {},
   "source": [
    "### Representing text\n",
    "After getting familiar with some basic tools, we will now look at how to transform text data into a numeric representation that can be used in more complex analysis. We will look at two ways that will offer different perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf071455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-a141dd82e43a>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  articles = pd.read_csv('articles.txt', header = None, sep='delimiter')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image copyright EPA Image caption Uber has bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ride-sharing firm Uber is facing a criminal in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scrutiny has started because the firm is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The software, called \"greyball\", helped it ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A spokesman for Uber declined to comment on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Image copyright EPA Image caption Uber has bee...\n",
       "1  Ride-sharing firm Uber is facing a criminal in...\n",
       "2  The scrutiny has started because the firm is a...\n",
       "3  The software, called \"greyball\", helped it ide...\n",
       "4  A spokesman for Uber declined to comment on th..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start loading a dataset\n",
    "# In the dataset we have 104 articles\n",
    "\n",
    "articles = pd.read_csv('articles.txt', header = None, sep='delimiter')\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c815c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image copyright epa image caption uber has bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ride-sharing firm uber is facing a criminal in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scrutiny has started because the firm is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the software, called \"greyball\", helped it ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a spokesman for uber declined to comment on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>the latest study shows that chatbots, driven b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>as the stilted computer interactions of today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>one concern is the potential for technology de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>there is also the potential for users to becom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>“however, there is also a huge potential for g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    image copyright epa image caption uber has bee...\n",
       "1    ride-sharing firm uber is facing a criminal in...\n",
       "2    the scrutiny has started because the firm is a...\n",
       "3    the software, called \"greyball\", helped it ide...\n",
       "4    a spokesman for uber declined to comment on th...\n",
       "..                                                 ...\n",
       "99   the latest study shows that chatbots, driven b...\n",
       "100  as the stilted computer interactions of today ...\n",
       "101  one concern is the potential for technology de...\n",
       "102  there is also the potential for users to becom...\n",
       "103  “however, there is also a huge potential for g...\n",
       "\n",
       "[104 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we will see the transformation functions include the possibility to perform basic transformation\n",
    "# but we can do it ourselves\n",
    "# First we can change everything to lowercase\n",
    "\n",
    "articles[0] = articles[0].str.lower()\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a4d803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image copyright epa image caption uber has bee...</td>\n",
       "      <td>[image, copyright, epa, image, caption, uber, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ride-sharing firm uber is facing a criminal in...</td>\n",
       "      <td>[ride-sharing, firm, uber, facing, criminal, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scrutiny has started because the firm is a...</td>\n",
       "      <td>[scrutiny, started, firm, accused, using, \"sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the software, called \"greyball\", helped it ide...</td>\n",
       "      <td>[software,, called, \"greyball\",, helped, ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a spokesman for uber declined to comment on th...</td>\n",
       "      <td>[spokesman, uber, declined, comment, investiga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  image copyright epa image caption uber has bee...   \n",
       "1  ride-sharing firm uber is facing a criminal in...   \n",
       "2  the scrutiny has started because the firm is a...   \n",
       "3  the software, called \"greyball\", helped it ide...   \n",
       "4  a spokesman for uber declined to comment on th...   \n",
       "\n",
       "                                               split  \n",
       "0  [image, copyright, epa, image, caption, uber, ...  \n",
       "1  [ride-sharing, firm, uber, facing, criminal, i...  \n",
       "2  [scrutiny, started, firm, accused, using, \"sec...  \n",
       "3  [software,, called, \"greyball\",, helped, ident...  \n",
       "4  [spokesman, uber, declined, comment, investiga...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next step is removing the stopwords\n",
    "# We still want to keep the string format and not create list so it requires some extra steps\n",
    "# If we just wanted to create a list from the words of ech article and then remove stopwords from that\n",
    "# we can do it easily using split\n",
    "\n",
    "articles['split'] = articles[0].apply(lambda x: x.split())\n",
    "\n",
    "# And then we can remove the stopwrods from the lists\n",
    "\n",
    "stop_words = list(stopwords.words(\"english\"))\n",
    "\n",
    "articles['split'] = articles['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67dbb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>split</th>\n",
       "      <th>split_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image copyright epa image caption uber has bee...</td>\n",
       "      <td>[image, copyright, epa, image, caption, uber, ...</td>\n",
       "      <td>[imag, copyright, epa, imag, caption, uber, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ride-sharing firm uber is facing a criminal in...</td>\n",
       "      <td>[ride-sharing, firm, uber, facing, criminal, i...</td>\n",
       "      <td>[ride-shar, firm, uber, face, crimin, investig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scrutiny has started because the firm is a...</td>\n",
       "      <td>[scrutiny, started, firm, accused, using, \"sec...</td>\n",
       "      <td>[scrutini, start, firm, accus, use, \"secret\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the software, called \"greyball\", helped it ide...</td>\n",
       "      <td>[software,, called, \"greyball\",, helped, ident...</td>\n",
       "      <td>[software,, call, \"greyball\",, help, identifi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a spokesman for uber declined to comment on th...</td>\n",
       "      <td>[spokesman, uber, declined, comment, investiga...</td>\n",
       "      <td>[spokesman, uber, declin, comment, investigati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>the latest study shows that chatbots, driven b...</td>\n",
       "      <td>[latest, study, shows, chatbots,, driven, mach...</td>\n",
       "      <td>[latest, studi, show, chatbots,, driven, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>as the stilted computer interactions of today ...</td>\n",
       "      <td>[stilted, computer, interactions, today, repla...</td>\n",
       "      <td>[stilt, comput, interact, today, replac, somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>one concern is the potential for technology de...</td>\n",
       "      <td>[one, concern, potential, technology, designed...</td>\n",
       "      <td>[one, concern, potenti, technolog, design, sed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>there is also the potential for users to becom...</td>\n",
       "      <td>[also, potential, users, become, emotionally, ...</td>\n",
       "      <td>[also, potenti, user, becom, emot, dependent,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>“however, there is also a huge potential for g...</td>\n",
       "      <td>[“however,, also, huge, potential, good,, exis...</td>\n",
       "      <td>[“however,, also, huge, potenti, good,, exist,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    image copyright epa image caption uber has bee...   \n",
       "1    ride-sharing firm uber is facing a criminal in...   \n",
       "2    the scrutiny has started because the firm is a...   \n",
       "3    the software, called \"greyball\", helped it ide...   \n",
       "4    a spokesman for uber declined to comment on th...   \n",
       "..                                                 ...   \n",
       "99   the latest study shows that chatbots, driven b...   \n",
       "100  as the stilted computer interactions of today ...   \n",
       "101  one concern is the potential for technology de...   \n",
       "102  there is also the potential for users to becom...   \n",
       "103  “however, there is also a huge potential for g...   \n",
       "\n",
       "                                                 split  \\\n",
       "0    [image, copyright, epa, image, caption, uber, ...   \n",
       "1    [ride-sharing, firm, uber, facing, criminal, i...   \n",
       "2    [scrutiny, started, firm, accused, using, \"sec...   \n",
       "3    [software,, called, \"greyball\",, helped, ident...   \n",
       "4    [spokesman, uber, declined, comment, investiga...   \n",
       "..                                                 ...   \n",
       "99   [latest, study, shows, chatbots,, driven, mach...   \n",
       "100  [stilted, computer, interactions, today, repla...   \n",
       "101  [one, concern, potential, technology, designed...   \n",
       "102  [also, potential, users, become, emotionally, ...   \n",
       "103  [“however,, also, huge, potential, good,, exis...   \n",
       "\n",
       "                                               split_1  \n",
       "0    [imag, copyright, epa, imag, caption, uber, cr...  \n",
       "1    [ride-shar, firm, uber, face, crimin, investig...  \n",
       "2    [scrutini, start, firm, accus, use, \"secret\", ...  \n",
       "3    [software,, call, \"greyball\",, help, identifi,...  \n",
       "4    [spokesman, uber, declin, comment, investigati...  \n",
       "..                                                 ...  \n",
       "99   [latest, studi, show, chatbots,, driven, machi...  \n",
       "100  [stilt, comput, interact, today, replac, somet...  \n",
       "101  [one, concern, potenti, technolog, design, sed...  \n",
       "102  [also, potenti, user, becom, emot, dependent,,...  \n",
       "103  [“however,, also, huge, potenti, good,, exist,...  \n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# we always need to initialize a stemmer object first\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "articles['split_1'] = articles['split'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6474a5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>split</th>\n",
       "      <th>split_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image copyright epa image caption uber has bee...</td>\n",
       "      <td>[image, copyright, epa, image, caption, uber, ...</td>\n",
       "      <td>[imag, copyright, epa, imag, caption, uber, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ride-sharing firm uber is facing a criminal in...</td>\n",
       "      <td>[ride-sharing, firm, uber, facing, criminal, i...</td>\n",
       "      <td>[ride-shar, firm, uber, face, crimin, investig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scrutiny has started because the firm is a...</td>\n",
       "      <td>[scrutiny, started, firm, accused, using, \"sec...</td>\n",
       "      <td>[scrutini, start, firm, accus, use, \"secret\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the software, called \"greyball\", helped it ide...</td>\n",
       "      <td>[software,, called, \"greyball\",, helped, ident...</td>\n",
       "      <td>[software,, call, \"greyball\",, help, identifi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a spokesman for uber declined to comment on th...</td>\n",
       "      <td>[spokesman, uber, declined, comment, investiga...</td>\n",
       "      <td>[spokesman, uber, declin, comment, investigati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>the latest study shows that chatbots, driven b...</td>\n",
       "      <td>[latest, study, shows, chatbots,, driven, mach...</td>\n",
       "      <td>[latest, studi, show, chatbots,, driven, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>as the stilted computer interactions of today ...</td>\n",
       "      <td>[stilted, computer, interactions, today, repla...</td>\n",
       "      <td>[stilt, comput, interact, today, replac, somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>one concern is the potential for technology de...</td>\n",
       "      <td>[one, concern, potential, technology, designed...</td>\n",
       "      <td>[one, concern, potenti, technolog, design, sed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>there is also the potential for users to becom...</td>\n",
       "      <td>[also, potential, users, become, emotionally, ...</td>\n",
       "      <td>[also, potenti, user, becom, emot, dependent,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>“however, there is also a huge potential for g...</td>\n",
       "      <td>[“however,, also, huge, potential, good,, exis...</td>\n",
       "      <td>[“however,, also, huge, potenti, good,, exist,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    image copyright epa image caption uber has bee...   \n",
       "1    ride-sharing firm uber is facing a criminal in...   \n",
       "2    the scrutiny has started because the firm is a...   \n",
       "3    the software, called \"greyball\", helped it ide...   \n",
       "4    a spokesman for uber declined to comment on th...   \n",
       "..                                                 ...   \n",
       "99   the latest study shows that chatbots, driven b...   \n",
       "100  as the stilted computer interactions of today ...   \n",
       "101  one concern is the potential for technology de...   \n",
       "102  there is also the potential for users to becom...   \n",
       "103  “however, there is also a huge potential for g...   \n",
       "\n",
       "                                                 split  \\\n",
       "0    [image, copyright, epa, image, caption, uber, ...   \n",
       "1    [ride-sharing, firm, uber, facing, criminal, i...   \n",
       "2    [scrutiny, started, firm, accused, using, \"sec...   \n",
       "3    [software,, called, \"greyball\",, helped, ident...   \n",
       "4    [spokesman, uber, declined, comment, investiga...   \n",
       "..                                                 ...   \n",
       "99   [latest, study, shows, chatbots,, driven, mach...   \n",
       "100  [stilted, computer, interactions, today, repla...   \n",
       "101  [one, concern, potential, technology, designed...   \n",
       "102  [also, potential, users, become, emotionally, ...   \n",
       "103  [“however,, also, huge, potential, good,, exis...   \n",
       "\n",
       "                                               split_1  \n",
       "0    [imag, copyright, epa, imag, caption, uber, cr...  \n",
       "1    [ride-shar, firm, uber, face, crimin, investig...  \n",
       "2    [scrutini, start, firm, accus, use, \"secret\", ...  \n",
       "3    [software,, call, \"greyball\",, help, identifi,...  \n",
       "4    [spokesman, uber, declin, comment, investigati...  \n",
       "..                                                 ...  \n",
       "99   [latest, studi, show, chatbots,, driven, machi...  \n",
       "100  [stilt, comput, interact, today, replac, somet...  \n",
       "101  [one, concern, potenti, technolog, design, sed...  \n",
       "102  [also, potenti, user, becom, emot, dependent,,...  \n",
       "103  [“however,, also, huge, potenti, good,, exist,...  \n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(word_list):\n",
    "    stemmed_list = []\n",
    "    for word in word_list:\n",
    "        stemmed_list.append(stemmer.stem(word))\n",
    "    return stemmed_list\n",
    "\n",
    "articles['split_1'] = articles['split'].apply(stem_words)\n",
    "articles       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008809d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then get back the single string format using join\n",
    "articles['split_final'] = articles['split'].apply(lambda x: ' '.join(x))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first representation we look as is called CountVectorizer and is available in the sklearn package\n",
    "# This will create a new dataset where each row corresponds to one of the original text documents (artciles) in this case\n",
    "# and each column is a word\n",
    "# Each value in the data then will provide the count on how many times the word (column) is present in the article (row)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We initialize an object \n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Then create the representation\n",
    "\n",
    "article_counts = vect.fit_transform(articles['split_final'])\n",
    "\n",
    "# We can check the size of the resulting data\n",
    "# We can see that we have 104 rows corresponding to the original articles, and 1442 columns for the words that \n",
    "# appear in at least 1 article\n",
    "\n",
    "article_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the data after converting it to a dataframe\n",
    "\n",
    "article_counts_df = pd.DataFrame(article_counts.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "article_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this table we can now do some basic word frequnecy analsys\n",
    "# First sum up the columns to get frequency and sort it\n",
    "# We can see that the most frequent word is said, which most likely not something we care about\n",
    "\n",
    "word_count = article_counts_df.sum(axis=0).sort_values(ascending = False)\n",
    "word_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we can go back to creating the representation with CountVectorizer as we can actually specify \n",
    "# removing stopwords there\n",
    "# We can make use of the parameter stop_words\n",
    "# If we set the value 'english', it will remove the most frequent english words\n",
    "# As we have already done that, it will not change anything \n",
    "# But we can extend that list as we have seen before \n",
    "stop_words_new = list(stopwords.words(\"english\")) + ['said']\n",
    "\n",
    "# We perform now the transformation\n",
    "\n",
    "vect_new = CountVectorizer(stop_words = stop_words_new)\n",
    "\n",
    "article_counts = vect_new.fit_transform(articles['split_final'])\n",
    "\n",
    "# We convert it to dataframe\n",
    "\n",
    "article_counts_df_new = pd.DataFrame(article_counts.toarray(), columns=vect_new.get_feature_names())\n",
    "\n",
    "# And look at the word count\n",
    "# As we can see said disappeared\n",
    "# We could run this again if we want to remove some further stopwords until we are satisfied\n",
    "\n",
    "word_count_new = article_counts_df_new.sum(axis=0).sort_values(ascending = False)\n",
    "word_count_new[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other representation is the term frequency inverse document frequency mentioned in the lecture\n",
    "# This will calculate a weighted frequency of the words\n",
    "# If a word appers too much, it will be recognized as not so relevant, as that word will \n",
    "# not help in differentiating the articles from each other, as it appears in a lot of them\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# We can also use stop_owrds if necessary\n",
    "\n",
    "tdf_vect = TfidfVectorizer()\n",
    "\n",
    "article_tdf = tdf_vect.fit_transform(articles['split_final'])\n",
    "\n",
    "# We convert it to dataframe\n",
    "\n",
    "article_tdf_df = pd.DataFrame(article_tdf.toarray(), columns=tdf_vect.get_feature_names())\n",
    "\n",
    "# And we can look at the sum of weights of a word (sum of values in a column)\n",
    "# As you can see it adjusted the count of said\n",
    "# The algorithm found that it appears too much, so it is actually not that important\n",
    "\n",
    "word_count_tdf = article_tdf_df.sum(axis=0).sort_values(ascending = False)\n",
    "word_count_tdf[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00318cb7",
   "metadata": {},
   "source": [
    "### Topic modeling\n",
    "We will look at one application, topic modeling. The idea is very similarto clustering. We try to group the text documents in a way that documents that end up in the same group are similar, i.e. they discuss more or less the same topic. We can run the very famous algorithm for this purpose: Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e291ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already created CountVectorizer, we can make use of the created object article_counts (not the dataframe)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# As in clustering, we need to determine the optimal number of topics\n",
    "# In this case I will not introduce any specific measure\n",
    "# You can try different values, and see whehter the resulting topics make sense\n",
    "# The algorithm also involves some randomness, so we need to fix it to get the same results when running again\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components = 3, random_state = 42)\n",
    "\n",
    "LDA_results = LDA.fit_transform(article_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf15a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are stored in an array, as the importance of each word in each topic\n",
    "# They way we try to understand topics is to print the most frequent words\n",
    "# If there are topics present in the data, the most frequent words should differe\n",
    "# and characterize topics in a meaningful way\n",
    "\n",
    "LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the 10 most important words for each topic\n",
    "\n",
    "for topic, component in enumerate(LDA.components_):\n",
    "    # first we \n",
    "    words_sorted = np.argsort(component)[-10:]\n",
    "    \n",
    "    print([vect_new.get_feature_names()[i] for i in words_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also add topic number to the original articles\n",
    "# In the outcome of LDA, each row tells us to what extent each article belongs to each topic\n",
    "# Based on the first row we can see that the first article seem to belong to the first topic\n",
    "LDA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can simply check which number is the higest, and will assign that topic\n",
    "LDA_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add this as a new column\n",
    "articles['Topic'] = LDA_results.argmax(axis=1)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf357fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
