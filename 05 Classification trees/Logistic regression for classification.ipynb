{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to classification with logistic regression\n",
    "\n",
    "After we have seen examples of performing regression modelling with different libraries, now we will look at the second type of predictive analysis problem, classification with python. From the general introduction to the topic, you already know the basics of logistic regression, working with training and test set, and performance evaluation, we will look at how to do this with the tools in the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, we will use data about Titanic, originally from Kaggle\n",
    "# https://www.kaggle.com/c/titanic\n",
    "\n",
    "titanic_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the data, we see the familiar variables\n",
    "# In this example, we only care about missing values as preparation\n",
    "# In particular, we have to fill in missing values for the Age variable\n",
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the median as the replacement value\n",
    "\n",
    "age_value = titanic_df['Age'].median()\n",
    "titanic_df['Age'] = titanic_df['Age'].fillna(age_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to make use of the Sex column, we turn into numeric \n",
    "# when 1 will correspond to females\n",
    "\n",
    "titanic_df['IsFemale'] = (titanic_df['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the prediction model, we will make use of three predictor variables, Pclass, Age and IsFemale\n",
    "# and we try to predict Survived, so we create a separate dataframe for this purpose\n",
    "\n",
    "prediction_data = titanic_df[['Pclass', 'IsFemale', 'Age', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to appropriately perform the model building, we need to create training and test set\n",
    "# For this purpose, we have a useful function available in sklearn, in model selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we crete training and test sets, we simply specify the dataframe, the outcome column\n",
    "# and what proportion of the data we want to be in the test set (in this example 20%)\n",
    "# This will create four things: \n",
    "# X_train and X_test: predictors for training and test set\n",
    "# y_train and y_test: outcome for training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(prediction_data[['Pclass', 'IsFemale', 'Age']], prediction_data.Survived, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can import the function that will build the model for us\n",
    "# and create the model object\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we can train the model using fit, with the predictor varibles and the outcome\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did with linear regression, we can alternatively use the statsmodels package\n",
    "# As we can observe from the coefficient, being  female has positive effect on survival\n",
    "# having higher class (in this case worse, as class 1 is the best) is negative,\n",
    "# and age has a small negative effect\n",
    "\n",
    "import statsmodels.api as sm\n",
    "logit_model = sm.Logit(y_train, X_train)\n",
    "result = logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a prediction for our test data based on the created model\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "y_predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to assess the quality of the model, we can look at the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_predict)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(93 + 47) /(93 + 15 + 24 + 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on this array, we could also manually calculate different measures\n",
    "# but there is is also a useful function for this\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_predict)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally an example of performing crossvalidation\n",
    "# When we look at the accuracy values, we can see quite big differences, \n",
    "# indicating that it relly impacts the results how the (single) test set is selected\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
